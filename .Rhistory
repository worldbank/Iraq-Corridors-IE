summarise(consumption_total = sum(consumption_total))
bisp_df <- merge(bisp_df, consum_sum_all, by = c("uid", "period"),
all.x=T, all.y=T)
# Export -----------------------------------------------------------------------
saveRDS(bisp_df, file.path(project_file_path, "Data", "BISP",
"FinalData", "Individual Datasets", "bisp_socioeconomic.Rds"))
write.csv(bisp_df, file.path(project_file_path, "Data", "BISP",
"FinalData", "Individual Datasets", "bisp_socioeconomic.csv"),
row.names = F)
# Load Data --------------------------------------------------------------------
bisp_df <- readRDS(file.path(project_file_path, "Data", "BISP", "FinalData",
"Individual Datasets", "bisp_socioeconomic.Rds"))
bisp_df$consumption_total
bisp_df$consumption_total %in% 0
table(bisp_df$consumption_total %in% 0)
median(bisp_df$consumption_total)
# Alluvial Diagram of Poverty Over Time
# Load Data --------------------------------------------------------------------
bisp_df <- readRDS(file.path(project_file_path, "Data", "BISP", "FinalData",
"Individual Datasets", "bisp_socioeconomic.Rds"))
## restrict to households with a poverty score in all four years
bisp_df <- bisp_df %>%
filter(!is.na(consumption_total)) %>%
dplyr::group_by(uid) %>%
dplyr::mutate(uid_count = n()) %>%
filter(uid_count %in% 4) %>%
ungroup()
## define poverty threshold
median(bisp_df$consumption_total)
## define poverty threshold
bisp_df$poverty <- as.numeric(bisp_df$consumption_total <= 62842.12)
table(bisp_df$poverty)
## Number of households
bisp_df$uid %>% unique() %>% length()
# Alluvial Diagram of Poverty Over Time
# Load Data --------------------------------------------------------------------
bisp_df <- readRDS(file.path(project_file_path, "Data", "BISP", "FinalData",
"Individual Datasets", "bisp_socioeconomic.Rds"))
## restrict to households with a poverty score in all four years
bisp_df <- bisp_df %>%
filter(!is.na(consumption_total)) %>%
dplyr::group_by(uid) %>%
dplyr::mutate(uid_count = n()) %>%
filter(uid_count %in% 4) %>%
ungroup()
## define poverty threshold
bisp_df$poor <- as.numeric(bisp_df$consumption_total <= 62842.12)
## Number of households
bisp_df$uid %>% unique() %>% length()
# Alluvial Diagram of Poverty Over Time
# Load Data --------------------------------------------------------------------
bisp_df <- readRDS(file.path(project_file_path, "Data", "BISP", "FinalData",
"Individual Datasets", "bisp_socioeconomic.Rds"))
## restrict to households with a poverty score in all four years
bisp_df <- bisp_df %>%
filter(!is.na(consumption_total)) %>%
dplyr::group_by(uid) %>%
dplyr::mutate(uid_count = n()) %>%
filter(uid_count %in% 4) %>%
ungroup()
## define poverty threshold
bisp_df$poor <- as.numeric(bisp_df$consumption_total <= 62842.12)
## Number of households
bisp_df$uid %>% unique() %>% length()
bisp_wide_df <- bisp_df %>%
dplyr::select(uid, year, poor) %>%
pivot_wider(names_from=year, values_from=poor) #%>%
#filter(!is.na(`2011`) & !is.na(`2013`) & !is.na(`2014`) & !is.na(`2016`))
head(bisp_wide_df)
# Alluvial Diagram of Poverty Over Time
# Load Data --------------------------------------------------------------------
bisp_df <- readRDS(file.path(project_file_path, "Data", "BISP", "FinalData",
"Individual Datasets", "bisp_socioeconomic.Rds"))
## restrict to households with a poverty score in all four years
bisp_df <- bisp_df %>%
filter(!is.na(consumption_total)) %>%
dplyr::group_by(uid) %>%
dplyr::mutate(uid_count = n()) %>%
filter(uid_count %in% 4) %>%
ungroup()
## define poverty threshold
bisp_df$poor <- ifelse(bisp_df$consumption_total <= 62842.12, "Below", "Above")
table(bisp_df$poor)
# Alluvial Diagram of Poverty Over Time
# Load Data --------------------------------------------------------------------
bisp_df <- readRDS(file.path(project_file_path, "Data", "BISP", "FinalData",
"Individual Datasets", "bisp_socioeconomic.Rds"))
## restrict to households with a poverty score in all four years
bisp_df <- bisp_df %>%
filter(!is.na(consumption_total)) %>%
dplyr::group_by(uid) %>%
dplyr::mutate(uid_count = n()) %>%
filter(uid_count %in% 4) %>%
ungroup()
## define poverty threshold
bisp_df$poor <- ifelse(bisp_df$consumption_total <= 62842.12, "Below", "Above") %>%
factor(levels=c( "Below","Above"))
table(bisp_df$poor)
# Alluvial Diagram of Poverty Over Time
# Load Data --------------------------------------------------------------------
bisp_df <- readRDS(file.path(project_file_path, "Data", "BISP", "FinalData",
"Individual Datasets", "bisp_socioeconomic.Rds"))
## restrict to households with a poverty score in all four years
bisp_df <- bisp_df %>%
filter(!is.na(consumption_total)) %>%
dplyr::group_by(uid) %>%
dplyr::mutate(uid_count = n()) %>%
filter(uid_count %in% 4) %>%
ungroup()
## define poverty threshold
bisp_df$poor <- ifelse(bisp_df$consumption_total <= 62842.12, "Below", "Above") %>%
factor(levels=c( "Below","Above"))
## Number of households
bisp_df$uid %>% unique() %>% length()
bisp_wide_df <- bisp_df %>%
dplyr::select(uid, year, poor) %>%
pivot_wider(names_from=year, values_from=poor)
head(bisp_wide_df)
bisp_wide_df$`2011`
bisp_wide_freq <- bisp_wide_df %>%
group_by(`2011`, `2013`, `2014`, `2016`) %>%
summarise(Freq = n())
head(bisp_wide_freq)
# Alluvial Diagram of Poverty Over Time
# Load Data --------------------------------------------------------------------
bisp_df <- readRDS(file.path(project_file_path, "Data", "BISP", "FinalData",
"Individual Datasets", "bisp_socioeconomic.Rds"))
## restrict to households with a poverty score in all four years
bisp_df <- bisp_df %>%
filter(!is.na(consumption_total)) %>%
dplyr::group_by(uid) %>%
dplyr::mutate(uid_count = n()) %>%
filter(uid_count %in% 4) %>%
ungroup()
## define poverty threshold
bisp_df$poor <- ifelse(bisp_df$consumption_total <= 62842.12, "Below", "Above") %>%
factor(levels=c( "Below","Above"))
## Number of households
bisp_df$uid %>% unique() %>% length()
bisp_wide_df <- bisp_df %>%
dplyr::select(uid, year, poor) %>%
pivot_wider(names_from=year, values_from=poor)
bisp_wide_freq <- bisp_wide_df %>%
group_by(`2011`, `2013`, `2014`, `2016`) %>%
summarise(Freq = n())
png(file.path(figures_file_path, "bisp_poverty_consumption_alluvial.png"), width=3000, height=1800, res=500)
alluvial(bisp_wide_freq[,1:4], freq=bisp_wide_freq$Freq,
col = ifelse(bisp_wide_freq$`2011` == "Above", "forestgreen", "gold3"),
border="white",
cex = 0.95,
alpha=.6,
cw=.15,
blocks=T,
axis_labels=c("2011", "2013", "2014", "2016")
)
dev.off()
# Load Data --------------------------------------------------------------------
bisp_plist <- read_dta(file.path(project_file_path, "Data", "BISP", "RawData - Deidentified", "bisp_combined_plist.dta"))
bisp_plist %>% View()
# Load Data --------------------------------------------------------------------
bisp_plist <- read_dta(file.path(project_file_path, "Data", "BISP", "RawData - Deidentified", "bisp_combined_plist.dta"))
# COVID Webscraping and Social Media Analysis
# Packages ---------------------------------------------------------------------
library(gtrendsR)
library(parallel)
library(pbmcapply)
library(ggplot2)
library(jsonlite)
library(stringr)
library(raster)
library(scales)
library(rmapshaper)
library(sparkline)
library(htmltools)
library(data.table)
library(plotly)
library(ISOcodes)
library(stringi)
library(lubridate)
library(purrr)
library(tidytext)
library(quanteda)
library(qdap)
library(SentimentAnalysis)
library(sentimentr)
library(tm)
library(tokenizers)
library(wordcloud)
library(ggwordcloud)
library(ggpubr)
library(dplyr)
library(sf)
library(geofacet)
library(readstata13)
library(strucchange)
library(forcats)
library(ISOcodes)
library(ggwordcloud)
library(hrbrthemes)
library(lexiconPT)
library(textdata)
library(tidyr)
library(rgeos)
library(dplyr)
library(tidylog)
library(TTR)
library(sparkline)
library(shinydashboard)
library(RColorBrewer)
library(shinythemes)
library(DT)
library(dplyr)
library(rmarkdown)
library(lubridate)
library(shiny)
library(wesanderson)
library(ggplot2)
library(tidyr)
library(shinyWidgets)
library(zoo)
library(bcrypt)
library(shinyjs)
library(ngram)
library(rtweet)
library(stringdist)
library(stringr)
library(rgdal)
library(rgeos)
library(geosphere)
library(htmlwidgets)
library(tidyverse)
library(sf)
library(tidyverse)
library(raster)
library(leaflet)
library(leaflet.extras)
library(plotly)
library(data.table)
library(formattable)
library(tidyr)
library(viridis)
library(data.table)
library(raster)
library(scales)
library(lubridate)
library(geosphere)
library(hrbrthemes)
# remotes::install_github("wilkelab/ggtext")
library(ggtext)
library(purrr)
# Filepaths --------------------------------------------------------------------
if(Sys.info()[["user"]] == "WB521633") dropbox_file_path <- "C:/Users/wb521633/Dropbox/World Bank/Side Work/COVID Social Media Analysis"
if(Sys.info()[["user"]] == "robmarty") dropbox_file_path <- "~/Dropbox/World Bank/Side Work/COVID Social Media Analysis"
if(Sys.info()[["user"]] == "wb537287") dropbox_file_path <- "/Users/wb537287/Dropbox/COVID Social Media Analysis"
if(Sys.info()[["user"]] == "manuelramos") dropbox_file_path <- "~/Dropbox/COVID Social Media Analysis"
if(Sys.info()[["user"]] == "WB521633") github_file_path <- "C:/Users/wb521633/Documents/Github/covid-social-media-analysis"
if(Sys.info()[["user"]] == "robmarty") github_file_path <- "~/Documents/Github/covid-social-media-analysis"
if(Sys.info()[["user"]] == "WB521633") covid_twitter_github <- "C:/Users/wb521633/Documents/Github/COVID-19-TweetIDs"
if(Sys.info()[["user"]] == "robmarty") covid_twitter_github <- "~/Documents/Github/COVID-19-TweetIDs"
brazil_twitter_figures_path <- file.path(dropbox_file_path, "Data", "twitter", "Outputs", "figures")
google_figures_path <- file.path(dropbox_file_path, "Data", "google_trends", "outputs", "figures")
# Scrapes google trends, using both a comparison state and without a comparison
# state
#### PARAMETERS
comparison_iso <- "US"
overwrite_files <- F
language <- "pt"
# Load file that indicates which language to use for each country. Contains
# a language code and country code
languages <- read.csv(file.path(dropbox_file_path,
"Data", "country_primary_language", "countries_lang.csv"),
stringsAsFactors = F)
language_codes_all <- languages$Language_code_main %>% unique()
language_codes_all <- language_codes_all[!is.na(language_codes_all)]
language_codes_all <- language_codes_all[language_codes_all != ""]
language_codes_all <- language_codes_all %>% sort()
for(language in language_codes_all){
# Terms to Scrape --------------------------------------------------------------
keywords <- readRDS(file.path(dropbox_file_path, "Data", "google_trends",
"keywords", "FinalData", "covid_keywords_alllanguages.Rds"))
keywords <- keywords %>%
arrange(priority_to_scrape) %>%
filter(scrape %in% "yes")
# Clean keyword
keywords_vec <- keywords[[paste0("keyword_", language)]] %>% tolower() %>% as.character()
keywords_vec <- keywords_vec[keywords_vec != ""]
keywords_vec <- keywords_vec %>% str_replace_all("\\n", "") # some have newline
# ISO Codes ------------------------------------------------------------------
# Grab iso/country codes where the selected language is the main language
iso2 <- languages$Code[languages$Language_code_main %in% language]
iso2 <- iso2[!is.na(iso2)]
# Function to Scrape Data ----------------------------------------------------
extract_trends <- function(iso_i,
term_i,
comparison_iso,
sleep_time = 0.01,
also_scrape_without_cstate = T){
print(iso_i)
#### 1. Scrape
# Without comparison state
if(also_scrape_without_cstate){
out <- gtrends(term_i,
geo = iso_i,
time = "2020-01-01 2020-09-20",
onlyInterest=T,
low_search_volume=T)
out_df <- out$interest_over_time
for(var in names(out_df)) out_df[[var]] <- out_df[[var]] %>% as.character()
}
# Didn't return error, but no hits? Object will be null, which will cause
# error later. In this case, we just want to skip.
if((class(out)[1] %in% "gtrends") & is.null(out_df)){
out_all_df <- NULL
} else{
# With comparison state
out_cstate <- gtrends(term_i,
geo = c(iso_i,
comparison_iso) %>%
unique(),
time = "2020-01-01 2020-09-20",
onlyInterest=T,
low_search_volume=T)
out_cstate_df <- out_cstate$interest_over_time
for(var in names(out_cstate_df)) out_cstate_df[[var]] <- out_cstate_df[[var]] %>% as.character()
#### 2. Prep comparison state output
out_cstate_df <- out_cstate_df %>%
dplyr::rename(hits_with_compstate = hits)
## Add hits of comparison state as variable (go from long to wide)
if(iso_i != comparison_iso){
# Grab hits of comparison state
out_cstate_compstate_df <- out_cstate_df %>%
filter(geo == comparison_iso) %>%
dplyr::select(date, hits_with_compstate) %>%
dplyr::rename(hits_compstate = hits_with_compstate)
# Restrict to state of interest (remove comparison state), and merge
# hits of comparison state
out_cstate_df <- out_cstate_df %>%
filter(geo != comparison_iso) %>%
left_join(out_cstate_compstate_df, by = "date")
} else{
out_cstate_df$hits_compstate = out_cstate_df$hits_with_compstate
}
#### 3. Merge datasets with comp state and without comp state
if(also_scrape_without_cstate){
out_all_df <- out_df %>%
dplyr::select(date, hits) %>%
left_join(out_cstate_df, by = "date")
} else{
out_all_df <- out_cstate_df
}
}
#### 4. Take a quick nap b/c of google rate limits
Sys.sleep(sleep_time) #  + runif(1)*2
print(nrow(out_all_df))
return(out_all_df)
}
# Scrape Data ------------------------------------------------------------------
# Nested for loop isn't ideal, but works so oh well.
for(term_i in keywords_vec){
for(iso_i in iso2){
out_path <- file.path(dropbox_file_path, "Data", "google_trends", "RawData",
"global_with_ref_state_by_keyword",
paste0("global_gtrends_ref_",
iso_i,
"_compr",
comparison_iso,
"_term",
term_i,
"_language",
language,
".Rds"))
if(!file.exists(out_path) | overwrite_files){
print(paste(iso_i, term_i, "-------------------------------------------"))
tryCatch({
term_df <- extract_trends(iso_i,
term_i,
comparison_iso)
term_df$language <- language
saveRDS(term_df, out_path)
Sys.sleep(0.01) # pause after each term
}, error=function(e){})
}
}
}
# end language loop
}
help(distinct)
# Scrapes google trends, using both a comparison state and without a comparison
# state
#### PARAMETERS
comparison_iso <- "US"
overwrite_files <- F
language <- "pt"
# Load file that indicates which language to use for each country. Contains
# a language code and country code
languages <- read.csv(file.path(dropbox_file_path,
"Data", "country_primary_language", "countries_lang.csv"),
stringsAsFactors = F)
language_codes_all <- languages$Language_code_main %>% unique()
language_codes_all <- language_codes_all[!is.na(language_codes_all)]
language_codes_all <- language_codes_all[language_codes_all != ""]
language_codes_all <- language_codes_all %>% sort()
for(language in language_codes_all){
# Terms to Scrape --------------------------------------------------------------
keywords <- readRDS(file.path(dropbox_file_path, "Data", "google_trends",
"keywords", "FinalData", "covid_keywords_alllanguages.Rds"))
keywords <- keywords %>%
arrange(priority_to_scrape) %>%
filter(scrape %in% "yes")
# Clean keyword
keywords_vec <- keywords[[paste0("keyword_", language)]] %>% tolower() %>% as.character()
keywords_vec <- keywords_vec[keywords_vec != ""]
keywords_vec <- keywords_vec %>% str_replace_all("\\n", "") # some have newline
# ISO Codes ------------------------------------------------------------------
# Grab iso/country codes where the selected language is the main language
iso2 <- languages$Code[languages$Language_code_main %in% language]
iso2 <- iso2[!is.na(iso2)]
# Function to Scrape Data ----------------------------------------------------
extract_trends <- function(iso_i,
term_i,
comparison_iso,
sleep_time = 0.01,
also_scrape_without_cstate = T){
print(iso_i)
#### 1. Scrape
# Without comparison state
if(also_scrape_without_cstate){
out <- gtrends(term_i,
geo = iso_i,
time = "2020-01-01 2020-09-20",
onlyInterest=T,
low_search_volume=T)
out_df <- out$interest_over_time
for(var in names(out_df)) out_df[[var]] <- out_df[[var]] %>% as.character()
}
# Didn't return error, but no hits? Object will be null, which will cause
# error later. In this case, we just want to skip.
if((class(out)[1] %in% "gtrends") & is.null(out_df)){
out_all_df <- NULL
} else{
# With comparison state
out_cstate <- gtrends(term_i,
geo = c(iso_i,
comparison_iso) %>%
unique(),
time = "2020-01-01 2020-09-20",
onlyInterest=T,
low_search_volume=T)
out_cstate_df <- out_cstate$interest_over_time
for(var in names(out_cstate_df)) out_cstate_df[[var]] <- out_cstate_df[[var]] %>% as.character()
#### 2. Prep comparison state output
out_cstate_df <- out_cstate_df %>%
dplyr::rename(hits_with_compstate = hits)
## Add hits of comparison state as variable (go from long to wide)
if(iso_i != comparison_iso){
# Grab hits of comparison state
out_cstate_compstate_df <- out_cstate_df %>%
filter(geo == comparison_iso) %>%
dplyr::select(date, hits_with_compstate) %>%
dplyr::rename(hits_compstate = hits_with_compstate)
# Restrict to state of interest (remove comparison state), and merge
# hits of comparison state
out_cstate_df <- out_cstate_df %>%
filter(geo != comparison_iso) %>%
left_join(out_cstate_compstate_df, by = "date")
} else{
out_cstate_df$hits_compstate = out_cstate_df$hits_with_compstate
}
#### 3. Merge datasets with comp state and without comp state
if(also_scrape_without_cstate){
out_all_df <- out_df %>%
dplyr::select(date, hits) %>%
left_join(out_cstate_df, by = "date")
} else{
out_all_df <- out_cstate_df
}
}
#### 4. Take a quick nap b/c of google rate limits
Sys.sleep(sleep_time) #  + runif(1)*2
print(nrow(out_all_df))
return(out_all_df)
}
# Scrape Data ------------------------------------------------------------------
# Nested for loop isn't ideal, but works so oh well.
for(term_i in keywords_vec){
for(iso_i in iso2){
out_path <- file.path(dropbox_file_path, "Data", "google_trends", "RawData",
"global_with_ref_state_by_keyword",
paste0("global_gtrends_ref_",
iso_i,
"_compr",
comparison_iso,
"_term",
term_i,
"_language",
language,
".Rds"))
if(!file.exists(out_path) | overwrite_files){
print(paste(iso_i, term_i, "-------------------------------------------"))
tryCatch({
term_df <- extract_trends(iso_i,
term_i,
comparison_iso)
term_df$language <- language
saveRDS(term_df, out_path)
Sys.sleep(0.01) # pause after each term
}, error=function(e){})
}
}
}
# end language loop
}
